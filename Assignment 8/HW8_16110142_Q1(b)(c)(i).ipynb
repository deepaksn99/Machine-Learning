{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(data):\n",
    "    return np.maximum(data,0)\n",
    "\n",
    "def sigmoid(data):\n",
    "    return (1/(1+np.exp(-data)))\n",
    "\n",
    "def softmax(data):\n",
    "    return np.exp(data)/(np.exp(data).sum())\n",
    "\n",
    "def loss_function(predicted, ground_truth):\n",
    "    return ((predicted - ground_truth)**2).sum()/len(predicted)\n",
    "\n",
    "def linear(data):\n",
    "    return data\n",
    "\n",
    "def ReLU_Deriv(data):\n",
    "    new_data = data[data>0]=1\n",
    "    new_data = new_data[data<0]=0\n",
    "    return new_data\n",
    "\n",
    "class neural_network():\n",
    "    \n",
    "    def __init__(self, n_layers, activation_list, hidden_units, data, loss, rseed=12):\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.activation_list = activation_list\n",
    "        self.loss = loss\n",
    "        self.bias = {}\n",
    "        self.weights = {}\n",
    "        self.data = data \n",
    "        self.hidden_units = hidden_units\n",
    "        np.random.seed(rseed)\n",
    "        self.derivative = {'weight':{},'bias':{}}\n",
    "        \n",
    "        for i in range(self.n_layers+1):\n",
    "            self.bias[i] = np.random.randn(self.hidden_units[i])\n",
    "            if i == 0:\n",
    "                self.weights[i] = np.random.randn(len(self.data[0]), self.hidden_units[i])\n",
    "            else:\n",
    "                self.weights[i] = np.random.randn(self.hidden_units[i-1], self.hidden_units[i])\n",
    "        \n",
    "    def forward(self, data):\n",
    "        \n",
    "        for i in range(self.n_layers+1):\n",
    "            if i==0:\n",
    "                temp = np.dot(data,self.weights[i])\n",
    "                if self.activation_list[i] == 'ReLU':\n",
    "                    temp = ReLU(temp)\n",
    "                elif self.activation_list[i]=='sigmoid':\n",
    "                    temp = sigmoid(temp)\n",
    "                else:\n",
    "                    temp = linear(temp)\n",
    "            else:\n",
    "                temp = np.dot(temp, self.weights[i])\n",
    "                if self.activation_list[i] =='ReLU':\n",
    "                    temp = ReLU(temp)\n",
    "                elif self.activation_list[i]=='sigmoid':\n",
    "                    temp = sigmoid(temp)\n",
    "                else:\n",
    "                    temp = linear(temp)\n",
    "        return temp\n",
    "    \n",
    "    def backpropagate(self, loss, y):\n",
    "        derivative_store = []\n",
    "        \n",
    "        if self.activation_list[-1]=='sigmoid':\n",
    "            fixed = [2/len(y)*(forward(self.data)-y)*forward(self.data)*(1-forward(self.data))]\n",
    "        if self.activation_list[-1]=='linear':\n",
    "            fixed = [2/len(y)*(forward(self.data)-y)]\n",
    "        \n",
    "        j=0\n",
    "        for i in range(self.n_layers+1):\n",
    "            index = self.n_layers-i\n",
    "            curr_activation = activation_list[index]\n",
    "            if curr_activation=='sigmoid':\n",
    "                temp1 = forward(self.data,till_layer= index)*(1-forward(self.data,till_layer=index))\n",
    "                self.derivative['weight'][index] = fixed[j]*forward(self.data, till_layer=index-1)\n",
    "                self.derivative['bias'][index] = fixed[j]\n",
    "                fixed.append(temp1*weights[index+1])\n",
    "                \n",
    "#                 temp1 = 2/len(y)*(y - forward(self.data,till_layer=index))\n",
    "#                 temp2 = forward(self.data)*(1-forward(self.data,till_layer=index))\n",
    "#                 temp3 = forward(self.data,till_layer=index-1)\n",
    "#                 self.derivative['weight'][index] = temp1*temp2*temp3\n",
    "            \n",
    "#             if curr_activation=='linear':\n",
    "#                 pass\n",
    "            j+=1\n",
    "                    \n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = neural_network(2, {0:ReLU, 1:sigmoid, 2:linear}, [4,2,1], np.array([[1,1,1],[2,2,2]]), loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.31366411],\n",
       "       [-1.15683205]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.forward(np.array([[2,2,2],[1,1,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function(np.array([1,1,1]),np.array([1,2,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for this part was majorly referred from:\n",
    "* The GitHub Repository [here](https://github.com/dennybritz/nn-from-scratch) and [here](https://github.com/pangolulu/neural-network-from-scratch).\n",
    "* The blogpost [here](https://jonathanweisbaerg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%201/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class_loss_function(y, predicted):\n",
    "    sum_loss = np.sum(np.multiply(y, np.log(predicted)))\n",
    "    return sum_loss/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "X = X/255 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "digits = 10\n",
    "number_of_examples = y.shape[0]\n",
    "y = y.reshape(1, number_of_examples)\n",
    "Y_new = np.eye(digits)[y.astype('int64')]\n",
    "Y_new = Y_new.T.reshape(digits, examples)\n",
    "## Setting the number of train samples\n",
    "m = 35000\n",
    "m_test = X.shape[0] - m\n",
    "X_train, X_test = X[:m].T, X[m:].T\n",
    "Y_train, Y_test = Y_new[:,:m], Y_new[:,m:]\n",
    "\n",
    "shuffle_index = np.random.permutation(m)\n",
    "X_train, Y_train = X_train[:, shuffle_index], Y_train[:, shuffle_index]\n",
    "a = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after  10  epochs -5644.2179493046915\n",
      "Cost after  20  epochs -3412.152710987341\n",
      "Cost after  30  epochs -2510.055502054724\n",
      "Cost after  40  epochs -2114.4536530916475\n",
      "Cost after  50  epochs -1871.2717479895175\n",
      "Cost after  60  epochs -1701.501553642613\n",
      "Cost after  70  epochs -1574.7095422877774\n",
      "Cost after  80  epochs -1475.4416970874363\n",
      "Cost after  90  epochs -1395.033959076589\n",
      "Cost after  100  epochs -1328.244175377284\n",
      "Cost after  110  epochs -1271.6728491126564\n",
      "Cost after  120  epochs -1222.9892642885548\n",
      "Cost after  130  epochs -1180.5307379866222\n",
      "Cost after  140  epochs -1143.0748528807126\n",
      "Cost after  150  epochs -1109.7013896108915\n",
      "Cost after  160  epochs -1079.7047467036932\n",
      "Cost after  170  epochs -1052.5356263916967\n",
      "Cost after  180  epochs -1027.7605493090593\n",
      "Cost after  190  epochs -1005.0330223831716\n",
      "Cost after  200  epochs -984.0727521644812\n",
      "Cost after  210  epochs -964.6505428159614\n",
      "Cost after  220  epochs -946.5772072020964\n",
      "Cost after  230  epochs -929.69528095693\n",
      "Cost after  240  epochs -913.8726849938199\n",
      "Cost after  250  epochs -898.9977668001917\n",
      "Cost after  260  epochs -884.9753652847257\n",
      "Cost after  270  epochs -871.7236822551504\n",
      "Cost after  280  epochs -859.1718129475132\n",
      "Cost after  290  epochs -847.2578125643198\n",
      "Cost after  300  epochs -835.927185100499\n",
      "Cost after  310  epochs -825.1316940112723\n",
      "Cost after  320  epochs -814.8284147893891\n",
      "Cost after  330  epochs -804.9789713332848\n",
      "Cost after  340  epochs -795.5489152033974\n",
      "Cost after  350  epochs -786.5072178345532\n",
      "Cost after  360  epochs -777.8258521564892\n",
      "Cost after  370  epochs -769.4794442760369\n",
      "Cost after  380  epochs -761.4449794365213\n",
      "Cost after  390  epochs -753.7015498565568\n",
      "Cost after  400  epochs -746.2301351352861\n",
      "Cost after  410  epochs -739.0134084789328\n",
      "Cost after  420  epochs -732.0355639784649\n",
      "Cost after  430  epochs -725.2821616122372\n",
      "Cost after  440  epochs -718.739987675364\n",
      "Cost after  450  epochs -712.3969290502985\n",
      "Cost after  460  epochs -706.2418601981973\n",
      "Cost after  470  epochs -700.2645420128986\n",
      "Cost after  480  epochs -694.4555317787708\n",
      "Cost after  490  epochs -688.8061034559861\n"
     ]
    }
   ],
   "source": [
    "n_x = X_train.shape[0]\n",
    "n_h = 64\n",
    "learning_rate = 1\n",
    "\n",
    "W1 = np.random.randn(n_h, n_x)\n",
    "b1 = np.zeros((n_h, 1))\n",
    "W2 = np.random.randn(digits, n_h)\n",
    "b2 = np.zeros((digits, 1))\n",
    "\n",
    "X = X_train\n",
    "Y = Y_train\n",
    "cost = 0\n",
    "for i in range(500):\n",
    "    if (i%10)==0 and i!=0:\n",
    "        print(\"Cost after \",i, \" epochs\", cost)\n",
    "    \n",
    "    Z1 = np.matmul(W1,X) + b1           ## FIRST LAYER OUTPUT\n",
    "    A1 = sigmoid(Z1)                    ## FIRST LAYER ACTIVATION\n",
    "    Z2 = np.matmul(W2,A1) + b2          ## SECOND LAYER INPUT\n",
    "    A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0) ## SECOND LAYER ACTIVATION\n",
    "\n",
    "    cost = multi_class_loss_function(Y, A2)   ## THE COST FUNCTION\n",
    "\n",
    "    dZ2 = A2-Y\n",
    "    dW2 = (1./m) * np.matmul(dZ2, A1.T)   ## COMPUTING THE DERIVATIVES\n",
    "    db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dA1 = np.matmul(W2.T, dZ2)            ## \n",
    "    dZ1 = dA1 * sigmoid(Z1) * (1 - sigmoid(Z1))\n",
    "    dW1 = (1./m) * np.matmul(dZ1, X.T)\n",
    "    db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    W2 = W2 - learning_rate * dW2          ## Gradient Descent Update\n",
    "    b2 = b2 - learning_rate * db2         ## Gradient Descent Update\n",
    "    W1 = W1 - learning_rate * dW1         ## Gradient Descent Update\n",
    "    b1 = b1 - learning_rate * db1          ## Gradient Descent Updates is here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below we can see the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>953</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "      <td>746</td>\n",
       "      <td>728</td>\n",
       "      <td>170</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1116</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>145</td>\n",
       "      <td>492</td>\n",
       "      <td>631</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>942</td>\n",
       "      <td>28</td>\n",
       "      <td>15</td>\n",
       "      <td>41</td>\n",
       "      <td>2684</td>\n",
       "      <td>1461</td>\n",
       "      <td>1405</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>919</td>\n",
       "      <td>5</td>\n",
       "      <td>140</td>\n",
       "      <td>91</td>\n",
       "      <td>1380</td>\n",
       "      <td>1566</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>948</td>\n",
       "      <td>27</td>\n",
       "      <td>2370</td>\n",
       "      <td>2731</td>\n",
       "      <td>682</td>\n",
       "      <td>5429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>1649</td>\n",
       "      <td>840</td>\n",
       "      <td>501</td>\n",
       "      <td>2371</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1    2    3    4     5     6     7     8     9\n",
       "0  953     0   15    6    4    41   746   728   170   197\n",
       "1    1  1116    9   10    3    11   145   492   631   106\n",
       "2    4     8  942   28   15    41  2684  1461  1405   177\n",
       "3    6     7   30  919    5   140    91  1380  1566   593\n",
       "4    3     2   20    8  948    27  2370  2731   682  5429\n",
       "5   13     2   16   39    7  1649   840   501  2371   456\n",
       "6    0     0    0    0    0     0     0     0     0     0\n",
       "7    0     0    0    0    0     0     0     0     0     0\n",
       "8    0     0    0    0    0     0     0     0     0     0\n",
       "9    0     0    0    0    0     0     0     0     0     0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1 = np.matmul(W1, X_test) + b1\n",
    "A1 = sigmoid(Z1)\n",
    "Z2 = np.matmul(W2, A1) + b2\n",
    "A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n",
    "predictions = np.argmax(A2, axis=0)\n",
    "labels = np.argmax(Y_test, axis=0)\n",
    "pd.DataFrame(confusion_matrix(predictions, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Test Error is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.33      0.50      2860\n",
      "          1       0.98      0.44      0.61      2524\n",
      "          2       0.91      0.14      0.24      6765\n",
      "          3       0.91      0.19      0.32      4737\n",
      "          4       0.97      0.08      0.14     12220\n",
      "          5       0.86      0.28      0.42      5894\n",
      "          6       0.00      0.00      0.00         0\n",
      "          7       0.00      0.00      0.00         0\n",
      "          8       0.00      0.00      0.00         0\n",
      "          9       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.93      0.19      0.30     35000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepak/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We achieve 30 % accuracy using this neural network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
